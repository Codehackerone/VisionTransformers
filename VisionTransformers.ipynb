{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXXazIsGtvLkJcYPexjzYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codehackerone/VisionTransformers/blob/main/VisionTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformers\n",
        "Read: https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2"
      ],
      "metadata": {
        "id": "KKZfONKp4Te0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing setup"
      ],
      "metadata": {
        "id": "JnIzo88r52FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4vzbXRO57DY",
        "outputId": "5d336127-a6cb-4d2a-abd9-b4be6339e617"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data\n",
        "\n"
      ],
      "metadata": {
        "id": "ANXOdPbp5RLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAs9yYw93vzZ",
        "outputId": "438c076e-3f89-4f3e-fa0f-6b25fd2e27d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n",
            "170508288/170498071 [==============================] - 14s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x_train: {x_train.shape} - y_train: {y_train.shape}\")\n",
        "print(f\"x_test: {x_test.shape} - y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2sta_N95UDy",
        "outputId": "81a2bd55-291e-4957-90cd-bf05ac19c003"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (50000, 32, 32, 3) - y_train: (50000, 1)\n",
            "x_test: (10000, 32, 32, 3) - y_test: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Hyperparameters"
      ],
      "metadata": {
        "id": "jDvS8qd_5cRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72\n",
        "patch_size = 6  \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [projection_dim * 2,projection_dim,] \n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ],
      "metadata": {
        "id": "HEoMhARW5Ziq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation"
      ],
      "metadata": {
        "id": "zh82puls5g8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(72, 72),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "FYhaEaUk5foD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing images"
      ],
      "metadata": {
        "id": "i6vYOZkR5oT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "_VCROC3H5mgv",
        "outputId": "6dc174da-6db8-48aa-ff12-9be369767ebe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Network\n"
      ],
      "metadata": {
        "id": "mp6aaqQJ6H9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building MLP Network"
      ],
      "metadata": {
        "id": "-RuDAYq56MWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CU2-ynNO5n4Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch Maker"
      ],
      "metadata": {
        "id": "JoJh_0uG6VNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        " \n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "\n",
        "# n = int(np.sqrt(patches.shape[1]))\n",
        "# plt.figure(figsize=(4, 4))\n",
        "# for i, patch in enumerate(patches[0]):\n",
        "#     ax = plt.subplot(n, n, i + 1)\n",
        "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "#     plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "#     plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "0MV-qxNi6RpV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch Encoder"
      ],
      "metadata": {
        "id": "zy1tMdxa79DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        " \n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "mSk-Svir6aiJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building vision Transformer"
      ],
      "metadata": {
        "id": "7CvGSj-q8AyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_classifier(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        " \n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        " \n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "NfnDL2O88DiM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling and Training"
      ],
      "metadata": {
        "id": "c7tjyJn18NeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the model"
      ],
      "metadata": {
        "id": "boN7ACxy8RPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_vit_classifier((32,32,3), 10)\n",
        "optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        " \n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\n",
        "       keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "       keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"), ],)"
      ],
      "metadata": {
        "id": "bGN_vqVe8Fsi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n"
      ],
      "metadata": {
        "id": "IJtBzoRv85PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=0.1,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAaJPNYZ8VSm",
        "outputId": "df9eccf6-4518-4a4b-a9af-47b128601982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 79s 397ms/step - loss: 2.0972 - accuracy: 0.3026 - top-5-accuracy: 0.7962 - val_loss: 1.5629 - val_accuracy: 0.4422 - val_top-5-accuracy: 0.9074\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 68s 388ms/step - loss: 1.5999 - accuracy: 0.4249 - top-5-accuracy: 0.8928 - val_loss: 1.3309 - val_accuracy: 0.5302 - val_top-5-accuracy: 0.9400\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 68s 388ms/step - loss: 1.4259 - accuracy: 0.4873 - top-5-accuracy: 0.9227 - val_loss: 1.1907 - val_accuracy: 0.5822 - val_top-5-accuracy: 0.9526\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 68s 388ms/step - loss: 1.3217 - accuracy: 0.5267 - top-5-accuracy: 0.9353 - val_loss: 1.1381 - val_accuracy: 0.6026 - val_top-5-accuracy: 0.9528\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 68s 389ms/step - loss: 1.2568 - accuracy: 0.5551 - top-5-accuracy: 0.9419 - val_loss: 1.0606 - val_accuracy: 0.6354 - val_top-5-accuracy: 0.9640\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 68s 388ms/step - loss: 1.1893 - accuracy: 0.5791 - top-5-accuracy: 0.9509 - val_loss: 1.0139 - val_accuracy: 0.6468 - val_top-5-accuracy: 0.9688\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 68s 389ms/step - loss: 1.1374 - accuracy: 0.5982 - top-5-accuracy: 0.9543 - val_loss: 0.9665 - val_accuracy: 0.6598 - val_top-5-accuracy: 0.9720\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 68s 389ms/step - loss: 1.0904 - accuracy: 0.6182 - top-5-accuracy: 0.9578 - val_loss: 0.9402 - val_accuracy: 0.6764 - val_top-5-accuracy: 0.9750\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 68s 389ms/step - loss: 1.0428 - accuracy: 0.6347 - top-5-accuracy: 0.9619 - val_loss: 0.8801 - val_accuracy: 0.6968 - val_top-5-accuracy: 0.9792\n",
            "Epoch 10/100\n",
            "124/176 [====================>.........] - ETA: 19s - loss: 1.0000 - accuracy: 0.6545 - top-5-accuracy: 0.9646"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Accuracy\n"
      ],
      "metadata": {
        "id": "nLZrq5gx88Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "jIbIELBy86jV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}