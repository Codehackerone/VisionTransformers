{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codehackerone/VisionTransformers/blob/main/Vision_Transformer_TF2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq5FKUqmzlFn",
        "outputId": "d4136af5-4c58-4e0d-9cf8-14b9ed700112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6LdPsk_nq5W4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JY3ik-6Iq5W7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        #attention takes three inputs: queries, keys, and values,\n",
        "        self.query_dense = Dense(embed_dim)\n",
        "        self.key_dense = Dense(embed_dim)\n",
        "        self.value_dense = Dense(embed_dim)\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        #use the product between the queries and the keys \n",
        "        #to know \"how much\" each element is the sequence is important with the rest\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        #resulting vector, score is divided by a scaling factor based on the size of the embedding\n",
        "        #scaling fcator is square root of the embeding dimension\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        #the attention scaled_score is then softmaxed\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        #Attention(Q, K, V ) = softmax[(QK)/√dim_key]V\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(\n",
        "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
        "        )\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "         \n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        #MSA takes the queries, keys, and values  as input from the previous layer \n",
        "        #and projects them using the three linear layers.\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )\n",
        "        #self attention of different heads are concatenated  \n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "imcFjMNsq5W9"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # Transfromer block multi-head Self Attention\n",
        "        self.multiheadselfattention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        out1 = self.layernorm1(inputs)       \n",
        "        attention_output = self.multiheadselfattention(out1)\n",
        "        attention_output = self.dropout1(attention_output, training=training)       \n",
        "        out2 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out2 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jyz3FveCq5W-"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        mlp_dim,\n",
        "        channels=3,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        # create patches based on patch_size\n",
        "        # image_size/patch_size==0\n",
        "        num_patches=self.create_patch(image_size,patch_size, channels)\n",
        "        self.d_model = d_model\n",
        "        self.rescale = Rescaling(1./255)\n",
        "        self.patch_proj= self.create_postional_embedding(num_patches, d_model)\n",
        "        self.enc_layers = [\n",
        "            TransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.mlp_head = tf.keras.Sequential(\n",
        "            [\n",
        "                Dense(mlp_dim, activation=tfa.activations.gelu),\n",
        "                Dropout(dropout),\n",
        "                Dense(num_classes),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def create_patch(self, image_size, patch_size, channels):\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = channels * patch_size ** 2\n",
        "        self.patch_size = patch_size\n",
        "        return num_patches\n",
        "    def create_postional_embedding(self,num_patches, d_model):\n",
        "        self.pos_emb = self.add_weight(\"pos_emb\", shape=(1, num_patches + 1, d_model))\n",
        "        self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
        "        print(self.class_emb.shape)\n",
        "        return Dense(d_model)\n",
        "   \n",
        "        \n",
        "    def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
        "        return patches\n",
        "\n",
        "    def call(self, x, training):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        #rescale \n",
        "        x = self.rescale(x)\n",
        "        # extract the patches from the image\n",
        "        patches = self.extract_patches(x)\n",
        "        # Apply the postio embedding\n",
        "        x = self.patch_proj(patches)        \n",
        "        class_emb = tf.broadcast_to(\n",
        "            self.class_emb, [batch_size, 1, self.d_model]\n",
        "        )              \n",
        "        x = tf.concat([class_emb, x], axis=1)\n",
        "        x = x + self.pos_emb        \n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training)\n",
        "        x = self.mlp_head(x[:, 0])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdg2op4mq5W_",
        "outputId": "c557a952-00dc-48ea-bdba-85cb3ba9f661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 64)\n",
            "Epoch 1/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3108 - accuracy: 0.1035"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 89s 95ms/step - loss: 2.3108 - accuracy: 0.1035 - val_loss: 2.3044 - val_accuracy: 0.1001 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 2.3063 - accuracy: 0.1010 - val_loss: 2.3044 - val_accuracy: 0.1001 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3059 - accuracy: 0.1009"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 84ms/step - loss: 2.3059 - accuracy: 0.1009 - val_loss: 2.3042 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.0997"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 85ms/step - loss: 2.3050 - accuracy: 0.0997 - val_loss: 2.3040 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3047 - accuracy: 0.0983"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.3047 - accuracy: 0.0983 - val_loss: 2.3037 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3048 - accuracy: 0.0980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 84ms/step - loss: 2.3048 - accuracy: 0.0980 - val_loss: 2.3037 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3041 - accuracy: 0.1001"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.3041 - accuracy: 0.1001 - val_loss: 2.3034 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3037 - accuracy: 0.1002"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 85ms/step - loss: 2.3037 - accuracy: 0.1001 - val_loss: 2.3033 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 2.3034 - accuracy: 0.0990 - val_loss: 2.3034 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3034 - accuracy: 0.0998"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.3034 - accuracy: 0.0998 - val_loss: 2.3030 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3032 - accuracy: 0.1000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 84ms/step - loss: 2.3032 - accuracy: 0.1000 - val_loss: 2.3028 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.3030 - accuracy: 0.0995"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.3030 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 2.3030 - accuracy: 0.0982 - val_loss: 2.3028 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 2.3029 - accuracy: 0.0990 - val_loss: 2.3029 - val_accuracy: 0.1000 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.2913 - accuracy: 0.1095"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.2913 - accuracy: 0.1094 - val_loss: 2.2300 - val_accuracy: 0.1802 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.1113 - accuracy: 0.1902"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 2.1112 - accuracy: 0.1902 - val_loss: 2.0085 - val_accuracy: 0.2100 - lr: 1.0000e-04\n",
            "Epoch 17/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.9591 - accuracy: 0.2162"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 84ms/step - loss: 1.9591 - accuracy: 0.2162 - val_loss: 1.9045 - val_accuracy: 0.2314 - lr: 1.0000e-04\n",
            "Epoch 18/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.8959 - accuracy: 0.2353"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 66s 84ms/step - loss: 1.8958 - accuracy: 0.2353 - val_loss: 1.8607 - val_accuracy: 0.2356 - lr: 1.0000e-04\n",
            "Epoch 19/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.8486 - accuracy: 0.2579"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 1.8486 - accuracy: 0.2580 - val_loss: 1.8058 - val_accuracy: 0.2785 - lr: 1.0000e-04\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 1.8189 - accuracy: 0.2803 - val_loss: 1.8078 - val_accuracy: 0.2867 - lr: 1.0000e-04\n",
            "Epoch 21/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.7893 - accuracy: 0.3064"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.7892 - accuracy: 0.3064 - val_loss: 1.7441 - val_accuracy: 0.3259 - lr: 1.0000e-04\n",
            "Epoch 22/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.7569 - accuracy: 0.3233"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.7569 - accuracy: 0.3233 - val_loss: 1.7090 - val_accuracy: 0.3427 - lr: 1.0000e-04\n",
            "Epoch 23/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.7278 - accuracy: 0.3363"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 84ms/step - loss: 1.7278 - accuracy: 0.3363 - val_loss: 1.6917 - val_accuracy: 0.3484 - lr: 1.0000e-04\n",
            "Epoch 24/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.7064 - accuracy: 0.3457"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.7064 - accuracy: 0.3457 - val_loss: 1.6862 - val_accuracy: 0.3482 - lr: 1.0000e-04\n",
            "Epoch 25/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6840 - accuracy: 0.3531"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6840 - accuracy: 0.3531 - val_loss: 1.6482 - val_accuracy: 0.3673 - lr: 1.0000e-04\n",
            "Epoch 26/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6647 - accuracy: 0.3590"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6647 - accuracy: 0.3590 - val_loss: 1.6221 - val_accuracy: 0.3778 - lr: 1.0000e-04\n",
            "Epoch 27/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6531 - accuracy: 0.3624"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6531 - accuracy: 0.3623 - val_loss: 1.6175 - val_accuracy: 0.3829 - lr: 1.0000e-04\n",
            "Epoch 28/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6371 - accuracy: 0.3715"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6371 - accuracy: 0.3715 - val_loss: 1.5987 - val_accuracy: 0.3785 - lr: 1.0000e-04\n",
            "Epoch 29/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6268 - accuracy: 0.3772"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6269 - accuracy: 0.3772 - val_loss: 1.5859 - val_accuracy: 0.3926 - lr: 1.0000e-04\n",
            "Epoch 30/30\n",
            "781/782 [============================>.] - ETA: 0s - loss: 1.6206 - accuracy: 0.3794"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as multi_head_self_attention_8_layer_call_fn, multi_head_self_attention_8_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn, layer_normalization_16_layer_call_and_return_conditional_losses, layer_normalization_17_layer_call_fn while saving (showing 5 of 144). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: weights/best.tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r782/782 [==============================] - 65s 83ms/step - loss: 1.6205 - accuracy: 0.3794 - val_loss: 1.5778 - val_accuracy: 0.3942 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "model = None\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "IMAGE_SIZE=32\n",
        "PATCH_SIZE=4 \n",
        "NUM_LAYERS=8\n",
        "NUM_HEADS=16\n",
        "MLP_DIM=128\n",
        "lr=0.001\n",
        "WEIGHT_DECAY=1e-4\n",
        "BATCH_SIZE=64\n",
        "epochs=30\n",
        "\n",
        "ds, info = tfds.load(\"cifar10\", as_supervised=True, with_info=True)\n",
        "ds_train = (\n",
        "    ds[\"train\"]\n",
        "    .cache()\n",
        "    .shuffle(1024)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "ds_test = (\n",
        "    ds[\"test\"]\n",
        "    .cache()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "\n",
        "model = VisionTransformer(\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=10,\n",
        "    d_model=64,\n",
        "    num_heads=NUM_HEADS,\n",
        "    mlp_dim=MLP_DIM,\n",
        "    channels=3,\n",
        "    dropout=0.1,\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tfa.optimizers.AdamW(learning_rate=lr, weight_decay=WEIGHT_DECAY),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10),\n",
        "mcp = tf.keras.callbacks.ModelCheckpoint(filepath='weights/best.tf', \n",
        "                                         save_best_only=True, \n",
        "                                         monitor='val_loss', \n",
        "                                         mode='min')\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                                 factor=0.1, \n",
        "                                                 patience=3, \n",
        "                                                 verbose=0, \n",
        "                                                 mode='auto',\n",
        "min_delta=0.0001, cooldown=0, min_lr=0)    \n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_test,\n",
        "    epochs=epochs,\n",
        "    callbacks=[early_stop, mcp, reduce_lr],\n",
        ")\n",
        "model.save_weights(os.path.join('.', \"vit\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f_TQVTMHq5XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3dfc44-6e31-4860-a199-155dab7d2a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vision_transformer_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_1 (Rescaling)     multiple                  0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            multiple                  3136      \n",
            "                                                                 \n",
            " transformer_block_8 (Transf  multiple                 33472     \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " transformer_block_9 (Transf  multiple                 33472     \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " transformer_block_10 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_11 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_12 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_13 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_14 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_15 (Trans  multiple                 33472     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " sequential_17 (Sequential)  (None, 10)                9610      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 284,746\n",
            "Trainable params: 284,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v8z2RaITq5XB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "a2f0b7a5-4ac8-4faa-fcd2-3cc388ce3b9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')\n",
        "#plot_model(model, to_file='model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Or7mWX0Gq5XB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "8d681bf4-9ebc-44a1-a4d2-9673ab9c62fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "IMAGE_SIZE=32\n",
        "PATCH_SIZE=8 \n",
        "NUM_LAYERS=8\n",
        "NUM_HEADS=16\n",
        "MLP_DIM=128\n",
        "lr=1e-3\n",
        "WEIGHT_DECAY=1e-4\n",
        "BATCH_SIZE=64\n",
        "num_patches=IMAGE_SIZE//PATCH_SIZE\n",
        "epochs=1\n",
        "ds = tfds.load(\"cifar10\", as_supervised=True)\n",
        "ds_train = (\n",
        "    ds[\"train\"]\n",
        "    .cache()\n",
        "    .shuffle(1024)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "for images, labels in ds_train.take(1):\n",
        "  for i in range(1):\n",
        "    #ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    \n",
        "    plt.axis(\"off\")\n",
        "patches = tf.image.extract_patches(\n",
        "            images=images[:1],\n",
        "            sizes=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "            strides=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "for imgs in patches:\n",
        "    count = 0\n",
        "    for r in range(num_patches):\n",
        "        for c in range(num_patches):\n",
        "            ax = plt.subplot(num_patches, num_patches, count+1)\n",
        "            plt.imshow(tf.reshape(imgs[r,c],shape=(PATCH_SIZE,PATCH_SIZE,3)).numpy().astype(\"uint8\"))\n",
        "            plt.axis(\"off\")\n",
        "            count += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IwKb6QdIq5XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bc2246-b36b-4e75-9c95-8089c12d7d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 1 6 8 7 9 5 0 2 2 7 4 0 1 6 0 8 8 5 1 4 6 5 4 7 8 4 8 1 8 6 0 7 4 2 0 6\n",
            " 1 8 8 1 5 8 5 6 9 2 8 5 6 5 5 1 0 4 6 1 4 5 8 5 4 4 1] tf.Tensor(\n",
            "[7 0 6 9 5 1 7 0 3 2 7 2 7 9 6 8 8 8 7 9 4 2 3 5 0 0 2 8 1 8 2 0 7 2 0 0 9\n",
            " 1 0 5 1 5 0 3 5 0 2 0 5 3 7 4 1 4 2 5 1 5 2 0 7 2 2 1], shape=(64,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for image, target in ds_test.take(1):\n",
        "  y = model.predict(image)\n",
        "  print(np.argmax(y, axis=1), target)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Vision_Transformer_TF2.3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}